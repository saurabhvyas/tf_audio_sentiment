{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notes : the model should be robust for both male and female speakers , and all age groups\n",
    "\n",
    "\n",
    "\n",
    "# v4 onwards use new dataset layout\n",
    "\n",
    "# To do \n",
    "\n",
    "# create dataset \n",
    "\n",
    "# download robust audio samples\n",
    "\n",
    "# create tensorflow model\n",
    "\n",
    "# create json dataset file , and dataloader\n",
    "\n",
    "# take maximum_frames as 2500 \n",
    "\n",
    "# ________________________________________________________\n",
    "\n",
    "# ideas \n",
    "\n",
    "# use MFCC, total energy and F0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/media/saurabh/New Volume/tf_audio_sentiment/pyAudioAnalysis')\n",
    "\n",
    "from  pyAudioAnalysis  import audioBasicIO\n",
    "from  pyAudioAnalysis import audioFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Fs, x] = audioBasicIO.readAudioFile(\"/media/saurabh/New Volume/tf_audio_sentiment/data/lizzie.wav\");\n",
    "F = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);\n",
    "\n",
    "print(len(F[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "from tensorflow.contrib import rnn\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "unhappy_directory_path='/media/saurabh/New Volume/tf_audio_sentiment/data_test/data/0/'\n",
    "unhappy_directory = os.fsencode(unhappy_directory_path)\n",
    "\n",
    "happy_directory_path='/media/saurabh/New Volume/tf_audio_sentiment/data/'\n",
    "happy_directory = os.fsencode(happy_directory_path)\n",
    "\n",
    "model_path = \"/media/saurabh/New Volume/tf_audio_sentiment/models/rnn_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audio_to_mfcc(fileurl):\n",
    "    rate, sig = wav.read(fileurl)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "#d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "#fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 25\n",
    "batch_size = 2\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps=2000 # max timesteps\n",
    "num_input = 13 # size of vector, at each timestep\n",
    "num_classes = 2 # happy or sad\n",
    "\n",
    "num_hidden = 110 # hidden layer num of features\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,timesteps, num_input]) \n",
    "y = tf.placeholder(tf.int32, [None]) #  Index of output , 0 = sad , 1 = happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function, that pads audio so that audio frames = max frames\n",
    "\n",
    "def pad(input):\n",
    "   # print(input.shape[0])\n",
    "     if input.shape[0] < timesteps:\n",
    "        \n",
    "        diff = timesteps - input.shape[0]\n",
    "        \n",
    "        # pad and return input\n",
    "        return np.pad(input,((0,diff),(0,0)), mode=\"constant\")\n",
    "    \n",
    "     elif input.shape[0] > timesteps:\n",
    "        \n",
    "        return input[:timesteps,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "        \n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logits = BiRNN(x, weights, biases)\n",
    "# prediction = tf.nn.softmax(logits) # use only for inference, softmax included in loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y_one_hot*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=Logits))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad007.wav\n",
      "(2000, 13)\n",
      "0.381441\n",
      "[[ 1.34701467  0.57999015]\n",
      " [ 1.34701467  0.57999015]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad025.wav\n",
      "(2000, 13)\n",
      "0.355337\n",
      "[[ 1.39219439  0.54043031]\n",
      " [ 1.39219439  0.54043031]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad001.wav\n",
      "(2000, 13)\n",
      "0.332098\n",
      "[[ 1.4348104   0.50312531]\n",
      " [ 1.4348104   0.50312531]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad003.wav\n",
      "(2000, 13)\n",
      "0.311325\n",
      "[[ 1.47509229  0.46787244]\n",
      " [ 1.47509229  0.46787244]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad005.wav\n",
      "(2000, 13)\n",
      "0.292683\n",
      "[[ 1.51324558  0.4344905 ]\n",
      " [ 1.51324558  0.43449047]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad008.wav\n",
      "(2000, 13)\n",
      "0.275891\n",
      "[[ 1.54945409  0.40281758]\n",
      " [ 1.54945397  0.40281758]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad010.wav\n",
      "(2000, 13)\n",
      "0.260707\n",
      "[[ 1.58388162  0.37270924]\n",
      " [ 1.58388162  0.37270921]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad012.wav\n",
      "(2000, 13)\n",
      "0.246932\n",
      "[[ 1.61667466  0.34403628]\n",
      " [ 1.61667466  0.34403625]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad014.wav\n",
      "(2000, 13)\n",
      "0.234391\n",
      "[[ 1.64796484  0.31668305]\n",
      " [ 1.64796484  0.31668305]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad017.wav\n",
      "(2000, 13)\n",
      "0.222938\n",
      "[[ 1.6778698   0.29054606]\n",
      " [ 1.6778698   0.29054603]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad019.wav\n",
      "(2000, 13)\n",
      "0.212446\n",
      "[[ 1.70649505  0.26553178]\n",
      " [ 1.70649517  0.26553181]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad021.wav\n",
      "(2000, 13)\n",
      "0.202808\n",
      "[[ 1.73393607  0.24155727]\n",
      " [ 1.73393607  0.24155727]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad023.wav\n",
      "(2000, 13)\n",
      "0.193929\n",
      "[[ 1.76027858  0.21854687]\n",
      " [ 1.76027858  0.21854687]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad026.wav\n",
      "(2000, 13)\n",
      "0.185729\n",
      "[[ 1.78559971  0.19643202]\n",
      " [ 1.78559983  0.19643202]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad028.wav\n",
      "(2000, 13)\n",
      "0.178136\n",
      "[[ 1.80996978  0.1751515 ]\n",
      " [ 1.80996978  0.1751515 ]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad030.wav\n",
      "(2000, 13)\n",
      "0.17109\n",
      "[[ 1.83345294  0.15464866]\n",
      " [ 1.83345294  0.15464869]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad032.wav\n",
      "(2000, 13)\n",
      "0.164536\n",
      "[[ 1.85610592  0.13487357]\n",
      " [ 1.85610592  0.13487354]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad034.wav\n",
      "(2000, 13)\n",
      "0.158427\n",
      "[[ 1.87798285  0.11577925]\n",
      " [ 1.87798285  0.11577931]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad007.wav\n",
      "(2000, 13)\n",
      "0.152721\n",
      "[[ 1.89913058  0.09732327]\n",
      " [ 1.89913058  0.09732324]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad025.wav\n",
      "(2000, 13)\n",
      "0.147383\n",
      "[[ 1.91959405  0.07946771]\n",
      " [ 1.91959405  0.07946771]]\n",
      "Raanjhanaa Last Scene Dialogue by Dhanush Heart Touching and Sad001.wav\n",
      "(2000, 13)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f581142444eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# train single \"unhappy\" example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                  _, c,logits = sess.run([optimizer, cost, Logits], feed_dict={x: [accoustic_features_1 , accoustic_features_2  ],\n\u001b[0;32m---> 26\u001b[0;31m                                                           y: [0,0]})\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        \n",
    "        it = iter(os.listdir(unhappy_directory))\n",
    "        for file in it:\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith(\".wav\"): \n",
    "                 print(filename)\n",
    "                 \n",
    "                 \n",
    "                 accoustic_features_1=pad(audio_to_mfcc(unhappy_directory_path + filename))\n",
    "                 accoustic_features_2=pad(audio_to_mfcc(unhappy_directory_path + os.fsdecode(next(it))) )\n",
    "                 \n",
    "                 \n",
    "                    \n",
    "                 print(accoustic_features_1.shape)\n",
    "                \n",
    "                 \n",
    "            \n",
    "                # train single \"unhappy\" example\n",
    "                 _, c,logits = sess.run([optimizer, cost, Logits], feed_dict={x: [accoustic_features_1 , accoustic_features_2  ],\n",
    "                                                          y: [0,0]})\n",
    "            \n",
    "                 \n",
    "                 print(c)\n",
    "                 print(logits)\n",
    "        \n",
    "                 continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "     \n",
    "            \n",
    "           \n",
    "        # Display logs per epoch step\n",
    "      #  if (epoch+1) % display_step == 0:\n",
    "       #     print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    \n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "          \n",
    "\n",
    "    # Test model\n",
    " #   correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "  #  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  #  print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accoustic_features=audio_to_mfcc( \"/home/saurabh/Documents/audio_classification/data/boy.wav\")\n",
    "\n",
    "print(accoustic_features.shape)\n",
    "\n",
    "new_features = pad(accoustic_features)\n",
    "\n",
    "print(new_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
