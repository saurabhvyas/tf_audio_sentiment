{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notes : the model should be robust for both male and female speakers , and all age groups\n",
    "\n",
    "\n",
    "\n",
    "# To do \n",
    "\n",
    "# create dataset \n",
    "\n",
    "# download robust audio samples\n",
    "\n",
    "# create tensorflow model\n",
    "\n",
    "# create json dataset file , and dataloader\n",
    "\n",
    "# take maximum_frames as 2500 \n",
    "\n",
    "# ________________________________________________________\n",
    "\n",
    "# ideas \n",
    "\n",
    "# use MFCC, total energy and F0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/media/saurabh/New Volume/tf_audio_sentiment/pyAudioAnalysis')\n",
    "\n",
    "from  pyAudioAnalysis  import audioBasicIO\n",
    "from  pyAudioAnalysis import audioFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Fs, x] = audioBasicIO.readAudioFile(\"/media/saurabh/New Volume/tf_audio_sentiment/data/lizzie.wav\");\n",
    "F = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);\n",
    "\n",
    "print(len(F[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "from tensorflow.contrib import rnn\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "directory_path='/media/saurabh/New Volume/tf_audio_sentiment/data/'\n",
    "directory = os.fsencode(directory_path)\n",
    "\n",
    "model_path = \"/media/saurabh/New Volume/tf_audio_sentiment/models/rnn_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audio_to_mfcc(fileurl):\n",
    "    rate, sig = wav.read(fileurl)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "#d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "#fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 25\n",
    "batch_size = 1\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps=2000 # max timesteps\n",
    "num_input = 13 # size of vector, at each timestep\n",
    "num_classes = 2 # happy or sad\n",
    "\n",
    "num_hidden = 110 # hidden layer num of features\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,timesteps, num_input]) \n",
    "y = tf.placeholder(tf.int32, [None]) #  Index of output , 0 = sad , 1 = happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function, that pads audio so that audio frames = max frames\n",
    "\n",
    "def pad(input):\n",
    "   # print(input.shape[0])\n",
    "     if input.shape[0] < timesteps:\n",
    "        \n",
    "        diff = timesteps - input.shape[0]\n",
    "        \n",
    "        # pad and return input\n",
    "        return np.pad(input,((0,diff),(0,0)), mode=\"constant\")\n",
    "    \n",
    "     elif input.shape[0] > timesteps:\n",
    "        \n",
    "        return input[:timesteps,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "        \n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logits = BiRNN(x, weights, biases)\n",
    "# prediction = tf.nn.softmax(logits) # use only for inference, softmax included in loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y_one_hot*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=Logits)\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 1.10584044]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.47890115]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 1.04440629]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.51274985]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.99341315]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.54308611]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.95110416]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.56996506]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.91598707]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.59356111]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.88681358]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.6141209]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.86254895]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.63192701]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.84234011]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.64727306]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.82548612]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.66044652]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.81141084]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.67171842]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.79964107]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.68133777]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.78978705]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.68952906]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.78152776]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.69649172]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.77459812]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.70240128]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.76877844]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.70741045]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.76388675]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.71165228]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.75977153]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.71524084]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.75630701]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.71827435]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.75338811]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72083688]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.75092733]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72299987]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.74885106]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72482473]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.74709797]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72636318]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.74561679]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72765958]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.74436426]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72875106]\n",
      "boy.wav\n",
      "(2000, 13)\n",
      "0\n",
      "[ 0.74330413]\n",
      "lizzie.wav\n",
      "(2000, 13)\n",
      "1\n",
      "[ 0.72966945]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        \n",
    "         # Loop over all files\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith(\".wav\"): \n",
    "                 print(filename)\n",
    "                 accoustic_features=pad(audio_to_mfcc(directory_path + filename))\n",
    "                 \n",
    "                 \n",
    "                    \n",
    "                 print(accoustic_features.shape)\n",
    "                \n",
    "                 txt_file=filename.replace('.wav','.txt')\n",
    "                 with open(directory_path + txt_file, 'r') as myfile:\n",
    "                    data=myfile.read().replace('\\n', '')\n",
    "                    y_digit=int(data)\n",
    "                    print(y_digit)\n",
    "            \n",
    "                    # Fit training using single example data\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: [accoustic_features],\n",
    "                                                          y: [y_digit]})\n",
    "            \n",
    "                 \n",
    "                    print(c)    \n",
    "        \n",
    "                 continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "     \n",
    "            \n",
    "           \n",
    "        # Display logs per epoch step\n",
    "      #  if (epoch+1) % display_step == 0:\n",
    "       #     print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    \n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "          \n",
    "\n",
    "    # Test model\n",
    " #   correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "  #  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  #  print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 13)\n",
      "(2000, 13)\n"
     ]
    }
   ],
   "source": [
    "accoustic_features=audio_to_mfcc( \"/home/saurabh/Documents/audio_classification/data/boy.wav\")\n",
    "\n",
    "print(accoustic_features.shape)\n",
    "\n",
    "new_features = pad(accoustic_features)\n",
    "\n",
    "print(new_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
